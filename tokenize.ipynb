{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set filepaths for text file and csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_filepath = \"../home/schen9/dataset.txt\"\n",
    "text_filepath = \"dataset/part_1.txt\"\n",
    "csv_filepath = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the text file and split it by double newline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading from dataset/part_1.txt\n"
     ]
    }
   ],
   "source": [
    "with open(text_filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as text_file:\n",
    "    rows = text_file.read().strip().split(\"\\n\\n\")\n",
    "\n",
    "print(f\"Finished reading from {text_filepath}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the text to the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a010f835ec684863af4ecf9db3309995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(csv_filepath, \"w\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Text\"]) # Header\n",
    "    for i, row in enumerate(tqdm(rows)):\n",
    "        writer.writerow([row.strip()])\n",
    "\n",
    "print(f\"Finished writing to {csv_filepath}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv onto a hugging face dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d16ef7a4b0d4275a57542675eecc0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a402680b7e4f88a2676cffabd9f595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/77240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text'],\n",
       "        num_rows: 76583\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\",data_files=csv_filepath) \n",
    "dataset = dataset.filter(lambda x: x[\"Text\"] is not None) # filter out NoneTypes\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "transformer_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize all the text in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888678274af242a9b2b27388788dedd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76583 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 76583\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Text\"], truncation=True)\n",
    "\n",
    "batch_size = 50\n",
    "tokens = dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f63461099043a39816a889b2097fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/446203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens.save_to_disk(\"my_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The White Monkey is a 1925 American silent dra...</td>\n",
       "      <td>[0, 133, 735, 34546, 16, 10, 36248, 470, 8454,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preservation\\nAn incomplete print of The White...</td>\n",
       "      <td>[0, 28917, 26481, 50118, 4688, 20044, 5780, 9,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>External links</td>\n",
       "      <td>[0, 47380, 5678, 2]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Films based on works by John Galsworthy\\nFilms...</td>\n",
       "      <td>[0, 36361, 4339, 716, 15, 1364, 30, 610, 272, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the montane grasslands and shrublands biome\\n ...</td>\n",
       "      <td>[0, 627, 27121, 1728, 6964, 8391, 8, 15383, 17...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76578</th>\n",
       "      <td>2000s\\nList of Pakistani films of 2000\\nList o...</td>\n",
       "      <td>[0, 17472, 29, 50118, 36583, 9, 9246, 3541, 9,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76579</th>\n",
       "      <td>2010s\\n List of Pakistani films of 2010\\n List...</td>\n",
       "      <td>[0, 24789, 29, 50118, 9527, 9, 9246, 3541, 9, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76580</th>\n",
       "      <td>2020s\\n List of Pakistani films of 2020\\n List...</td>\n",
       "      <td>[0, 24837, 29, 50118, 9527, 9, 9246, 3541, 9, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76581</th>\n",
       "      <td>See also \\n Cinema of Pakistan\\n List of Pakis...</td>\n",
       "      <td>[0, 19224, 67, 1437, 50118, 20036, 9, 1752, 50...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76582</th>\n",
       "      <td>External links\\n Search Pakistani film - IMDB....</td>\n",
       "      <td>[0, 47380, 5678, 50118, 12180, 9246, 822, 111,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76583 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      The White Monkey is a 1925 American silent dra...   \n",
       "1      Preservation\\nAn incomplete print of The White...   \n",
       "2                                         External links   \n",
       "3      Films based on works by John Galsworthy\\nFilms...   \n",
       "4      the montane grasslands and shrublands biome\\n ...   \n",
       "...                                                  ...   \n",
       "76578  2000s\\nList of Pakistani films of 2000\\nList o...   \n",
       "76579  2010s\\n List of Pakistani films of 2010\\n List...   \n",
       "76580  2020s\\n List of Pakistani films of 2020\\n List...   \n",
       "76581  See also \\n Cinema of Pakistan\\n List of Pakis...   \n",
       "76582  External links\\n Search Pakistani film - IMDB....   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [0, 133, 735, 34546, 16, 10, 36248, 470, 8454,...   \n",
       "1      [0, 28917, 26481, 50118, 4688, 20044, 5780, 9,...   \n",
       "2                                    [0, 47380, 5678, 2]   \n",
       "3      [0, 36361, 4339, 716, 15, 1364, 30, 610, 272, ...   \n",
       "4      [0, 627, 27121, 1728, 6964, 8391, 8, 15383, 17...   \n",
       "...                                                  ...   \n",
       "76578  [0, 17472, 29, 50118, 36583, 9, 9246, 3541, 9,...   \n",
       "76579  [0, 24789, 29, 50118, 9527, 9, 9246, 3541, 9, ...   \n",
       "76580  [0, 24837, 29, 50118, 9527, 9, 9246, 3541, 9, ...   \n",
       "76581  [0, 19224, 67, 1437, 50118, 20036, 9, 1752, 50...   \n",
       "76582  [0, 47380, 5678, 50118, 12180, 9246, 822, 111,...   \n",
       "\n",
       "                                          attention_mask  \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2                                           [1, 1, 1, 1]  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "76578  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "76579  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "76580  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "76581  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "76582  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[76583 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [tensor(0, dtype=torch.int32), tensor(133, dty...\n",
       "1        [tensor(0, dtype=torch.int32), tensor(28917, d...\n",
       "2        [tensor(0, dtype=torch.int32), tensor(47380, d...\n",
       "3        [tensor(0, dtype=torch.int32), tensor(36361, d...\n",
       "4        [tensor(0, dtype=torch.int32), tensor(627, dty...\n",
       "                               ...                        \n",
       "76578    [tensor(0, dtype=torch.int32), tensor(17472, d...\n",
       "76579    [tensor(0, dtype=torch.int32), tensor(24789, d...\n",
       "76580    [tensor(0, dtype=torch.int32), tensor(24837, d...\n",
       "76581    [tensor(0, dtype=torch.int32), tensor(19224, d...\n",
       "76582    [tensor(0, dtype=torch.int32), tensor(47380, d...\n",
       "Name: tensors, Length: 76583, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tokens[\"train\"].to_pandas()\n",
    "df[\"tensors\"] = df[\"input_ids\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"tensors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 76583\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10975, 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĠAmerican'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(transformer_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(tokens[\"train\"], batch_size=20)\n",
    "token_embeddings = {i: [] for i in range(tokenizer.vocab_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 76583\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [],\n",
       " 2: [],\n",
       " 3: [],\n",
       " 4: [],\n",
       " 5: [],\n",
       " 6: [],\n",
       " 7: [],\n",
       " 8: [],\n",
       " 9: [],\n",
       " 10: [],\n",
       " 11: [],\n",
       " 12: [],\n",
       " 13: [],\n",
       " 14: [],\n",
       " 15: [],\n",
       " 16: [],\n",
       " 17: [],\n",
       " 18: [],\n",
       " 19: [],\n",
       " 20: [],\n",
       " 21: [],\n",
       " 22: [],\n",
       " 23: [],\n",
       " 24: [],\n",
       " 25: [],\n",
       " 26: [],\n",
       " 27: [],\n",
       " 28: [],\n",
       " 29: [],\n",
       " 30: [],\n",
       " 31: [],\n",
       " 32: [],\n",
       " 33: [],\n",
       " 34: [],\n",
       " 35: [],\n",
       " 36: [],\n",
       " 37: [],\n",
       " 38: [],\n",
       " 39: [],\n",
       " 40: [],\n",
       " 41: [],\n",
       " 42: [],\n",
       " 43: [],\n",
       " 44: [],\n",
       " 45: [],\n",
       " 46: [],\n",
       " 47: [],\n",
       " 48: [],\n",
       " 49: [],\n",
       " 50: [],\n",
       " 51: [],\n",
       " 52: [],\n",
       " 53: [],\n",
       " 54: [],\n",
       " 55: [],\n",
       " 56: [],\n",
       " 57: [],\n",
       " 58: [],\n",
       " 59: [],\n",
       " 60: [],\n",
       " 61: [],\n",
       " 62: [],\n",
       " 63: [],\n",
       " 64: [],\n",
       " 65: [],\n",
       " 66: [],\n",
       " 67: [],\n",
       " 68: [],\n",
       " 69: [],\n",
       " 70: [],\n",
       " 71: [],\n",
       " 72: [],\n",
       " 73: [],\n",
       " 74: [],\n",
       " 75: [],\n",
       " 76: [],\n",
       " 77: [],\n",
       " 78: [],\n",
       " 79: [],\n",
       " 80: [],\n",
       " 81: [],\n",
       " 82: [],\n",
       " 83: [],\n",
       " 84: [],\n",
       " 85: [],\n",
       " 86: [],\n",
       " 87: [],\n",
       " 88: [],\n",
       " 89: [],\n",
       " 90: [],\n",
       " 91: [],\n",
       " 92: [],\n",
       " 93: [],\n",
       " 94: [],\n",
       " 95: [],\n",
       " 96: [],\n",
       " 97: [],\n",
       " 98: [],\n",
       " 99: [],\n",
       " 100: [],\n",
       " 101: [],\n",
       " 102: [],\n",
       " 103: [],\n",
       " 104: [],\n",
       " 105: [],\n",
       " 106: [],\n",
       " 107: [],\n",
       " 108: [],\n",
       " 109: [],\n",
       " 110: [],\n",
       " 111: [],\n",
       " 112: [],\n",
       " 113: [],\n",
       " 114: [],\n",
       " 115: [],\n",
       " 116: [],\n",
       " 117: [],\n",
       " 118: [],\n",
       " 119: [],\n",
       " 120: [],\n",
       " 121: [],\n",
       " 122: [],\n",
       " 123: [],\n",
       " 124: [],\n",
       " 125: [],\n",
       " 126: [],\n",
       " 127: [],\n",
       " 128: [],\n",
       " 129: [],\n",
       " 130: [],\n",
       " 131: [],\n",
       " 132: [],\n",
       " 133: [],\n",
       " 134: [],\n",
       " 135: [],\n",
       " 136: [],\n",
       " 137: [],\n",
       " 138: [],\n",
       " 139: [],\n",
       " 140: [],\n",
       " 141: [],\n",
       " 142: [],\n",
       " 143: [],\n",
       " 144: [],\n",
       " 145: [],\n",
       " 146: [],\n",
       " 147: [],\n",
       " 148: [],\n",
       " 149: [],\n",
       " 150: [],\n",
       " 151: [],\n",
       " 152: [],\n",
       " 153: [],\n",
       " 154: [],\n",
       " 155: [],\n",
       " 156: [],\n",
       " 157: [],\n",
       " 158: [],\n",
       " 159: [],\n",
       " 160: [],\n",
       " 161: [],\n",
       " 162: [],\n",
       " 163: [],\n",
       " 164: [],\n",
       " 165: [],\n",
       " 166: [],\n",
       " 167: [],\n",
       " 168: [],\n",
       " 169: [],\n",
       " 170: [],\n",
       " 171: [],\n",
       " 172: [],\n",
       " 173: [],\n",
       " 174: [],\n",
       " 175: [],\n",
       " 176: [],\n",
       " 177: [],\n",
       " 178: [],\n",
       " 179: [],\n",
       " 180: [],\n",
       " 181: [],\n",
       " 182: [],\n",
       " 183: [],\n",
       " 184: [],\n",
       " 185: [],\n",
       " 186: [],\n",
       " 187: [],\n",
       " 188: [],\n",
       " 189: [],\n",
       " 190: [],\n",
       " 191: [],\n",
       " 192: [],\n",
       " 193: [],\n",
       " 194: [],\n",
       " 195: [],\n",
       " 196: [],\n",
       " 197: [],\n",
       " 198: [],\n",
       " 199: [],\n",
       " 200: [],\n",
       " 201: [],\n",
       " 202: [],\n",
       " 203: [],\n",
       " 204: [],\n",
       " 205: [],\n",
       " 206: [],\n",
       " 207: [],\n",
       " 208: [],\n",
       " 209: [],\n",
       " 210: [],\n",
       " 211: [],\n",
       " 212: [],\n",
       " 213: [],\n",
       " 214: [],\n",
       " 215: [],\n",
       " 216: [],\n",
       " 217: [],\n",
       " 218: [],\n",
       " 219: [],\n",
       " 220: [],\n",
       " 221: [],\n",
       " 222: [],\n",
       " 223: [],\n",
       " 224: [],\n",
       " 225: [],\n",
       " 226: [],\n",
       " 227: [],\n",
       " 228: [],\n",
       " 229: [],\n",
       " 230: [],\n",
       " 231: [],\n",
       " 232: [],\n",
       " 233: [],\n",
       " 234: [],\n",
       " 235: [],\n",
       " 236: [],\n",
       " 237: [],\n",
       " 238: [],\n",
       " 239: [],\n",
       " 240: [],\n",
       " 241: [],\n",
       " 242: [],\n",
       " 243: [],\n",
       " 244: [],\n",
       " 245: [],\n",
       " 246: [],\n",
       " 247: [],\n",
       " 248: [],\n",
       " 249: [],\n",
       " 250: [],\n",
       " 251: [],\n",
       " 252: [],\n",
       " 253: [],\n",
       " 254: [],\n",
       " 255: [],\n",
       " 256: [],\n",
       " 257: [],\n",
       " 258: [],\n",
       " 259: [],\n",
       " 260: [],\n",
       " 261: [],\n",
       " 262: [],\n",
       " 263: [],\n",
       " 264: [],\n",
       " 265: [],\n",
       " 266: [],\n",
       " 267: [],\n",
       " 268: [],\n",
       " 269: [],\n",
       " 270: [],\n",
       " 271: [],\n",
       " 272: [],\n",
       " 273: [],\n",
       " 274: [],\n",
       " 275: [],\n",
       " 276: [],\n",
       " 277: [],\n",
       " 278: [],\n",
       " 279: [],\n",
       " 280: [],\n",
       " 281: [],\n",
       " 282: [],\n",
       " 283: [],\n",
       " 284: [],\n",
       " 285: [],\n",
       " 286: [],\n",
       " 287: [],\n",
       " 288: [],\n",
       " 289: [],\n",
       " 290: [],\n",
       " 291: [],\n",
       " 292: [],\n",
       " 293: [],\n",
       " 294: [],\n",
       " 295: [],\n",
       " 296: [],\n",
       " 297: [],\n",
       " 298: [],\n",
       " 299: [],\n",
       " 300: [],\n",
       " 301: [],\n",
       " 302: [],\n",
       " 303: [],\n",
       " 304: [],\n",
       " 305: [],\n",
       " 306: [],\n",
       " 307: [],\n",
       " 308: [],\n",
       " 309: [],\n",
       " 310: [],\n",
       " 311: [],\n",
       " 312: [],\n",
       " 313: [],\n",
       " 314: [],\n",
       " 315: [],\n",
       " 316: [],\n",
       " 317: [],\n",
       " 318: [],\n",
       " 319: [],\n",
       " 320: [],\n",
       " 321: [],\n",
       " 322: [],\n",
       " 323: [],\n",
       " 324: [],\n",
       " 325: [],\n",
       " 326: [],\n",
       " 327: [],\n",
       " 328: [],\n",
       " 329: [],\n",
       " 330: [],\n",
       " 331: [],\n",
       " 332: [],\n",
       " 333: [],\n",
       " 334: [],\n",
       " 335: [],\n",
       " 336: [],\n",
       " 337: [],\n",
       " 338: [],\n",
       " 339: [],\n",
       " 340: [],\n",
       " 341: [],\n",
       " 342: [],\n",
       " 343: [],\n",
       " 344: [],\n",
       " 345: [],\n",
       " 346: [],\n",
       " 347: [],\n",
       " 348: [],\n",
       " 349: [],\n",
       " 350: [],\n",
       " 351: [],\n",
       " 352: [],\n",
       " 353: [],\n",
       " 354: [],\n",
       " 355: [],\n",
       " 356: [],\n",
       " 357: [],\n",
       " 358: [],\n",
       " 359: [],\n",
       " 360: [],\n",
       " 361: [],\n",
       " 362: [],\n",
       " 363: [],\n",
       " 364: [],\n",
       " 365: [],\n",
       " 366: [],\n",
       " 367: [],\n",
       " 368: [],\n",
       " 369: [],\n",
       " 370: [],\n",
       " 371: [],\n",
       " 372: [],\n",
       " 373: [],\n",
       " 374: [],\n",
       " 375: [],\n",
       " 376: [],\n",
       " 377: [],\n",
       " 378: [],\n",
       " 379: [],\n",
       " 380: [],\n",
       " 381: [],\n",
       " 382: [],\n",
       " 383: [],\n",
       " 384: [],\n",
       " 385: [],\n",
       " 386: [],\n",
       " 387: [],\n",
       " 388: [],\n",
       " 389: [],\n",
       " 390: [],\n",
       " 391: [],\n",
       " 392: [],\n",
       " 393: [],\n",
       " 394: [],\n",
       " 395: [],\n",
       " 396: [],\n",
       " 397: [],\n",
       " 398: [],\n",
       " 399: [],\n",
       " 400: [],\n",
       " 401: [],\n",
       " 402: [],\n",
       " 403: [],\n",
       " 404: [],\n",
       " 405: [],\n",
       " 406: [],\n",
       " 407: [],\n",
       " 408: [],\n",
       " 409: [],\n",
       " 410: [],\n",
       " 411: [],\n",
       " 412: [],\n",
       " 413: [],\n",
       " 414: [],\n",
       " 415: [],\n",
       " 416: [],\n",
       " 417: [],\n",
       " 418: [],\n",
       " 419: [],\n",
       " 420: [],\n",
       " 421: [],\n",
       " 422: [],\n",
       " 423: [],\n",
       " 424: [],\n",
       " 425: [],\n",
       " 426: [],\n",
       " 427: [],\n",
       " 428: [],\n",
       " 429: [],\n",
       " 430: [],\n",
       " 431: [],\n",
       " 432: [],\n",
       " 433: [],\n",
       " 434: [],\n",
       " 435: [],\n",
       " 436: [],\n",
       " 437: [],\n",
       " 438: [],\n",
       " 439: [],\n",
       " 440: [],\n",
       " 441: [],\n",
       " 442: [],\n",
       " 443: [],\n",
       " 444: [],\n",
       " 445: [],\n",
       " 446: [],\n",
       " 447: [],\n",
       " 448: [],\n",
       " 449: [],\n",
       " 450: [],\n",
       " 451: [],\n",
       " 452: [],\n",
       " 453: [],\n",
       " 454: [],\n",
       " 455: [],\n",
       " 456: [],\n",
       " 457: [],\n",
       " 458: [],\n",
       " 459: [],\n",
       " 460: [],\n",
       " 461: [],\n",
       " 462: [],\n",
       " 463: [],\n",
       " 464: [],\n",
       " 465: [],\n",
       " 466: [],\n",
       " 467: [],\n",
       " 468: [],\n",
       " 469: [],\n",
       " 470: [],\n",
       " 471: [],\n",
       " 472: [],\n",
       " 473: [],\n",
       " 474: [],\n",
       " 475: [],\n",
       " 476: [],\n",
       " 477: [],\n",
       " 478: [],\n",
       " 479: [],\n",
       " 480: [],\n",
       " 481: [],\n",
       " 482: [],\n",
       " 483: [],\n",
       " 484: [],\n",
       " 485: [],\n",
       " 486: [],\n",
       " 487: [],\n",
       " 488: [],\n",
       " 489: [],\n",
       " 490: [],\n",
       " 491: [],\n",
       " 492: [],\n",
       " 493: [],\n",
       " 494: [],\n",
       " 495: [],\n",
       " 496: [],\n",
       " 497: [],\n",
       " 498: [],\n",
       " 499: [],\n",
       " 500: [],\n",
       " 501: [],\n",
       " 502: [],\n",
       " 503: [],\n",
       " 504: [],\n",
       " 505: [],\n",
       " 506: [],\n",
       " 507: [],\n",
       " 508: [],\n",
       " 509: [],\n",
       " 510: [],\n",
       " 511: [],\n",
       " 512: [],\n",
       " 513: [],\n",
       " 514: [],\n",
       " 515: [],\n",
       " 516: [],\n",
       " 517: [],\n",
       " 518: [],\n",
       " 519: [],\n",
       " 520: [],\n",
       " 521: [],\n",
       " 522: [],\n",
       " 523: [],\n",
       " 524: [],\n",
       " 525: [],\n",
       " 526: [],\n",
       " 527: [],\n",
       " 528: [],\n",
       " 529: [],\n",
       " 530: [],\n",
       " 531: [],\n",
       " 532: [],\n",
       " 533: [],\n",
       " 534: [],\n",
       " 535: [],\n",
       " 536: [],\n",
       " 537: [],\n",
       " 538: [],\n",
       " 539: [],\n",
       " 540: [],\n",
       " 541: [],\n",
       " 542: [],\n",
       " 543: [],\n",
       " 544: [],\n",
       " 545: [],\n",
       " 546: [],\n",
       " 547: [],\n",
       " 548: [],\n",
       " 549: [],\n",
       " 550: [],\n",
       " 551: [],\n",
       " 552: [],\n",
       " 553: [],\n",
       " 554: [],\n",
       " 555: [],\n",
       " 556: [],\n",
       " 557: [],\n",
       " 558: [],\n",
       " 559: [],\n",
       " 560: [],\n",
       " 561: [],\n",
       " 562: [],\n",
       " 563: [],\n",
       " 564: [],\n",
       " 565: [],\n",
       " 566: [],\n",
       " 567: [],\n",
       " 568: [],\n",
       " 569: [],\n",
       " 570: [],\n",
       " 571: [],\n",
       " 572: [],\n",
       " 573: [],\n",
       " 574: [],\n",
       " 575: [],\n",
       " 576: [],\n",
       " 577: [],\n",
       " 578: [],\n",
       " 579: [],\n",
       " 580: [],\n",
       " 581: [],\n",
       " 582: [],\n",
       " 583: [],\n",
       " 584: [],\n",
       " 585: [],\n",
       " 586: [],\n",
       " 587: [],\n",
       " 588: [],\n",
       " 589: [],\n",
       " 590: [],\n",
       " 591: [],\n",
       " 592: [],\n",
       " 593: [],\n",
       " 594: [],\n",
       " 595: [],\n",
       " 596: [],\n",
       " 597: [],\n",
       " 598: [],\n",
       " 599: [],\n",
       " 600: [],\n",
       " 601: [],\n",
       " 602: [],\n",
       " 603: [],\n",
       " 604: [],\n",
       " 605: [],\n",
       " 606: [],\n",
       " 607: [],\n",
       " 608: [],\n",
       " 609: [],\n",
       " 610: [],\n",
       " 611: [],\n",
       " 612: [],\n",
       " 613: [],\n",
       " 614: [],\n",
       " 615: [],\n",
       " 616: [],\n",
       " 617: [],\n",
       " 618: [],\n",
       " 619: [],\n",
       " 620: [],\n",
       " 621: [],\n",
       " 622: [],\n",
       " 623: [],\n",
       " 624: [],\n",
       " 625: [],\n",
       " 626: [],\n",
       " 627: [],\n",
       " 628: [],\n",
       " 629: [],\n",
       " 630: [],\n",
       " 631: [],\n",
       " 632: [],\n",
       " 633: [],\n",
       " 634: [],\n",
       " 635: [],\n",
       " 636: [],\n",
       " 637: [],\n",
       " 638: [],\n",
       " 639: [],\n",
       " 640: [],\n",
       " 641: [],\n",
       " 642: [],\n",
       " 643: [],\n",
       " 644: [],\n",
       " 645: [],\n",
       " 646: [],\n",
       " 647: [],\n",
       " 648: [],\n",
       " 649: [],\n",
       " 650: [],\n",
       " 651: [],\n",
       " 652: [],\n",
       " 653: [],\n",
       " 654: [],\n",
       " 655: [],\n",
       " 656: [],\n",
       " 657: [],\n",
       " 658: [],\n",
       " 659: [],\n",
       " 660: [],\n",
       " 661: [],\n",
       " 662: [],\n",
       " 663: [],\n",
       " 664: [],\n",
       " 665: [],\n",
       " 666: [],\n",
       " 667: [],\n",
       " 668: [],\n",
       " 669: [],\n",
       " 670: [],\n",
       " 671: [],\n",
       " 672: [],\n",
       " 673: [],\n",
       " 674: [],\n",
       " 675: [],\n",
       " 676: [],\n",
       " 677: [],\n",
       " 678: [],\n",
       " 679: [],\n",
       " 680: [],\n",
       " 681: [],\n",
       " 682: [],\n",
       " 683: [],\n",
       " 684: [],\n",
       " 685: [],\n",
       " 686: [],\n",
       " 687: [],\n",
       " 688: [],\n",
       " 689: [],\n",
       " 690: [],\n",
       " 691: [],\n",
       " 692: [],\n",
       " 693: [],\n",
       " 694: [],\n",
       " 695: [],\n",
       " 696: [],\n",
       " 697: [],\n",
       " 698: [],\n",
       " 699: [],\n",
       " 700: [],\n",
       " 701: [],\n",
       " 702: [],\n",
       " 703: [],\n",
       " 704: [],\n",
       " 705: [],\n",
       " 706: [],\n",
       " 707: [],\n",
       " 708: [],\n",
       " 709: [],\n",
       " 710: [],\n",
       " 711: [],\n",
       " 712: [],\n",
       " 713: [],\n",
       " 714: [],\n",
       " 715: [],\n",
       " 716: [],\n",
       " 717: [],\n",
       " 718: [],\n",
       " 719: [],\n",
       " 720: [],\n",
       " 721: [],\n",
       " 722: [],\n",
       " 723: [],\n",
       " 724: [],\n",
       " 725: [],\n",
       " 726: [],\n",
       " 727: [],\n",
       " 728: [],\n",
       " 729: [],\n",
       " 730: [],\n",
       " 731: [],\n",
       " 732: [],\n",
       " 733: [],\n",
       " 734: [],\n",
       " 735: [],\n",
       " 736: [],\n",
       " 737: [],\n",
       " 738: [],\n",
       " 739: [],\n",
       " 740: [],\n",
       " 741: [],\n",
       " 742: [],\n",
       " 743: [],\n",
       " 744: [],\n",
       " 745: [],\n",
       " 746: [],\n",
       " 747: [],\n",
       " 748: [],\n",
       " 749: [],\n",
       " 750: [],\n",
       " 751: [],\n",
       " 752: [],\n",
       " 753: [],\n",
       " 754: [],\n",
       " 755: [],\n",
       " 756: [],\n",
       " 757: [],\n",
       " 758: [],\n",
       " 759: [],\n",
       " 760: [],\n",
       " 761: [],\n",
       " 762: [],\n",
       " 763: [],\n",
       " 764: [],\n",
       " 765: [],\n",
       " 766: [],\n",
       " 767: [],\n",
       " 768: [],\n",
       " 769: [],\n",
       " 770: [],\n",
       " 771: [],\n",
       " 772: [],\n",
       " 773: [],\n",
       " 774: [],\n",
       " 775: [],\n",
       " 776: [],\n",
       " 777: [],\n",
       " 778: [],\n",
       " 779: [],\n",
       " 780: [],\n",
       " 781: [],\n",
       " 782: [],\n",
       " 783: [],\n",
       " 784: [],\n",
       " 785: [],\n",
       " 786: [],\n",
       " 787: [],\n",
       " 788: [],\n",
       " 789: [],\n",
       " 790: [],\n",
       " 791: [],\n",
       " 792: [],\n",
       " 793: [],\n",
       " 794: [],\n",
       " 795: [],\n",
       " 796: [],\n",
       " 797: [],\n",
       " 798: [],\n",
       " 799: [],\n",
       " 800: [],\n",
       " 801: [],\n",
       " 802: [],\n",
       " 803: [],\n",
       " 804: [],\n",
       " 805: [],\n",
       " 806: [],\n",
       " 807: [],\n",
       " 808: [],\n",
       " 809: [],\n",
       " 810: [],\n",
       " 811: [],\n",
       " 812: [],\n",
       " 813: [],\n",
       " 814: [],\n",
       " 815: [],\n",
       " 816: [],\n",
       " 817: [],\n",
       " 818: [],\n",
       " 819: [],\n",
       " 820: [],\n",
       " 821: [],\n",
       " 822: [],\n",
       " 823: [],\n",
       " 824: [],\n",
       " 825: [],\n",
       " 826: [],\n",
       " 827: [],\n",
       " 828: [],\n",
       " 829: [],\n",
       " 830: [],\n",
       " 831: [],\n",
       " 832: [],\n",
       " 833: [],\n",
       " 834: [],\n",
       " 835: [],\n",
       " 836: [],\n",
       " 837: [],\n",
       " 838: [],\n",
       " 839: [],\n",
       " 840: [],\n",
       " 841: [],\n",
       " 842: [],\n",
       " 843: [],\n",
       " 844: [],\n",
       " 845: [],\n",
       " 846: [],\n",
       " 847: [],\n",
       " 848: [],\n",
       " 849: [],\n",
       " 850: [],\n",
       " 851: [],\n",
       " 852: [],\n",
       " 853: [],\n",
       " 854: [],\n",
       " 855: [],\n",
       " 856: [],\n",
       " 857: [],\n",
       " 858: [],\n",
       " 859: [],\n",
       " 860: [],\n",
       " 861: [],\n",
       " 862: [],\n",
       " 863: [],\n",
       " 864: [],\n",
       " 865: [],\n",
       " 866: [],\n",
       " 867: [],\n",
       " 868: [],\n",
       " 869: [],\n",
       " 870: [],\n",
       " 871: [],\n",
       " 872: [],\n",
       " 873: [],\n",
       " 874: [],\n",
       " 875: [],\n",
       " 876: [],\n",
       " 877: [],\n",
       " 878: [],\n",
       " 879: [],\n",
       " 880: [],\n",
       " 881: [],\n",
       " 882: [],\n",
       " 883: [],\n",
       " 884: [],\n",
       " 885: [],\n",
       " 886: [],\n",
       " 887: [],\n",
       " 888: [],\n",
       " 889: [],\n",
       " 890: [],\n",
       " 891: [],\n",
       " 892: [],\n",
       " 893: [],\n",
       " 894: [],\n",
       " 895: [],\n",
       " 896: [],\n",
       " 897: [],\n",
       " 898: [],\n",
       " 899: [],\n",
       " 900: [],\n",
       " 901: [],\n",
       " 902: [],\n",
       " 903: [],\n",
       " 904: [],\n",
       " 905: [],\n",
       " 906: [],\n",
       " 907: [],\n",
       " 908: [],\n",
       " 909: [],\n",
       " 910: [],\n",
       " 911: [],\n",
       " 912: [],\n",
       " 913: [],\n",
       " 914: [],\n",
       " 915: [],\n",
       " 916: [],\n",
       " 917: [],\n",
       " 918: [],\n",
       " 919: [],\n",
       " 920: [],\n",
       " 921: [],\n",
       " 922: [],\n",
       " 923: [],\n",
       " 924: [],\n",
       " 925: [],\n",
       " 926: [],\n",
       " 927: [],\n",
       " 928: [],\n",
       " 929: [],\n",
       " 930: [],\n",
       " 931: [],\n",
       " 932: [],\n",
       " 933: [],\n",
       " 934: [],\n",
       " 935: [],\n",
       " 936: [],\n",
       " 937: [],\n",
       " 938: [],\n",
       " 939: [],\n",
       " 940: [],\n",
       " 941: [],\n",
       " 942: [],\n",
       " 943: [],\n",
       " 944: [],\n",
       " 945: [],\n",
       " 946: [],\n",
       " 947: [],\n",
       " 948: [],\n",
       " 949: [],\n",
       " 950: [],\n",
       " 951: [],\n",
       " 952: [],\n",
       " 953: [],\n",
       " 954: [],\n",
       " 955: [],\n",
       " 956: [],\n",
       " 957: [],\n",
       " 958: [],\n",
       " 959: [],\n",
       " 960: [],\n",
       " 961: [],\n",
       " 962: [],\n",
       " 963: [],\n",
       " 964: [],\n",
       " 965: [],\n",
       " 966: [],\n",
       " 967: [],\n",
       " 968: [],\n",
       " 969: [],\n",
       " 970: [],\n",
       " 971: [],\n",
       " 972: [],\n",
       " 973: [],\n",
       " 974: [],\n",
       " 975: [],\n",
       " 976: [],\n",
       " 977: [],\n",
       " 978: [],\n",
       " 979: [],\n",
       " 980: [],\n",
       " 981: [],\n",
       " 982: [],\n",
       " 983: [],\n",
       " 984: [],\n",
       " 985: [],\n",
       " 986: [],\n",
       " 987: [],\n",
       " 988: [],\n",
       " 989: [],\n",
       " 990: [],\n",
       " 991: [],\n",
       " 992: [],\n",
       " 993: [],\n",
       " 994: [],\n",
       " 995: [],\n",
       " 996: [],\n",
       " 997: [],\n",
       " 998: [],\n",
       " 999: [],\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca5e00090714b6cb21ea3b698679362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Extract input IDs and attention masks\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 171\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[0;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:207\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    205\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Extract input IDs and attention masks\n",
    "        print(batch)\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        \n",
    "        # Get model outputs (last hidden state)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Iterate over tokens in the batch\n",
    "        for i, seq in enumerate(input_ids):\n",
    "            for j, token_id in enumerate(seq):\n",
    "                # Skip padding tokens\n",
    "                if attention_mask[i, j].item() == 0:\n",
    "                    continue\n",
    "                token_embeddings[token_id.item()].append(hidden_states[i, j].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
